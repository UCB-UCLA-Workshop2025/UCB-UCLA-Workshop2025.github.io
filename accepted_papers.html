<!Doctype html>
<html lang="en">
    <head>
        <title>AI3DG: Workshop on AI for 3D Generation</title>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="author" content="Despoina Paschalidou">
        <meta name="description" content="Workshop page for ai3dcc">
        <!-- Bootstrap -->
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" type="text/css" href="style.css?cache=77333914184988801948">
        <link rel="icon" type="image/png" href="figures/favicon.png"/>
    </head>
    <body>
        <div id="navbar-top">
        <nav class="navbar navbar-light px-3">
            <a class="navbar-brand" href="https://cvpr.thecvf.com"/>AI3DG @ CVPR2024</a>
            <ul class="nav nav-pills">
              <li class="nav-item">
                <a class="nav-link active" href="index.html#home">Home</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="index.html#submission">Submission</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="index.html#schedule">Schedule</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="accepted_papers.html">
                Accepted Papers</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="index.html#speakers">Speakers</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="program_committee.html">
                Program Committee</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="index.html#organizers">Organizers</a>
              </li>
            </ul>
        </nav>
        </div>
        <div data-bs-spy="scroll" data-bs-target="#navbar-example2" data-bs-offset="0" class="scrollspy-example" tabindex="0">

        <div class="jumbotron">
            <div class="papers-title">
                <h2>Workshop Publications</h2>
            </div>
            <!-- start paper list --><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/pdf/2406.01136">Towards Practical Single-shot Motion Synthesis</a></div><div class="authors">Roditakis,  Konstantinos; Thermos,  Spyridon; Zioulis Nikolaos</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="papers/paper_15.pdf" data-type="Paper">Paper</a> <a href="posters/poster_15.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Despite the recent advances in the so-called 'cold start' generation from text prompts, their needs in data and computing resources, as well as the ambiguities around intellectual property and privacy concerns pose certain counterarguments for their utility. An interesting and relatively unexplored alternative has been the introduction of unconditional synthesis from a single sample, which has led to interesting generative applications. In this paper we focus on single-shot motion generation and more specifically on accelerating the training time of a Generative Adversarial Network (GAN). In particular, we tackle the challenge of GAN's equilibrium collapse when using mini-batch training by carefully annealing the weights of the loss functions that prevent mode collapse. Additionally, we perform statistical analysis in the generator and discriminator models to identify correlations between training stages and enable transfer learning. Our improved GAN achieves competitive quality and diversity on the Mixamo benchmark when compared to the original GAN architecture and a single-shot diffusion model, while being up to x6.8 faster in training time from the former and x1.75 from the latter. Finally, we demonstrate the ability of our improved GAN to mix and compose motion with a single forward pass.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2406.07742">C3DAG: Controlled 3D Animal Generation using 3D pose guidance</a></div><div class="authors">Mishra,  Sandeep; Saha,  Oindrila ; Bovik,  Alan</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2406.07742" data-type="Paper">Paper</a> <a href="posters/poster_27.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Recent advancements in text-to-3D generation have demonstrated the ability to generate high quality 3D assets. However while generating animals these methods underperform, often portraying inaccurate anatomy and geometry. Towards ameliorating this defect, we present C3DAG, a novel pose-Controlled text-to-3D Animal Generation framework which generates a high quality 3D animal consistent with a given pose. We also introduce an automatic 3D shape creator tool, that allows dynamic pose generation and modification via a web-based tool, and that generates a 3D balloon animal using simple geometries. A NeRF is then initialized using this 3D shape using depth-controlled SDS. In the next stage, the pre-trained NeRF is fine-tuned using quadruped-pose-controlled SDS. The pipeline that we have developed not only produces geometrically and anatomically consistent results, but also renders highly controlled 3D animals, unlike prior methods which do not allow fine-grained pose control.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2312.00451">FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</a></div><div class="authors">Zhu,  Zehao; Fan,  Zhiwen; Jiang,  Yifan; Cong,  Wenyan; Wang,  Zhangyang</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2312.00451" data-type="Paper">Paper</a> <a href="posters/poster_41.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Novel view synthesis from limited observations remains an important and persistent task. However, high efficiency in existing NeRF-based few-shot view synthesis is often compromised to obtain an accurate 3D representation. To address this challenge, we propose a few-shot view synthesis framework based on 3D Gaussian Splatting that enables real-time and photo-realistic view synthesis with as few as three training views. The proposed method, dubbed FSGS, handles the extremely sparse initialized SfM points with a thoughtfully designed Gaussian Unpooling process. Our method iteratively distributes new Gaussians around the most representative locations, subsequently infilling local details in vacant areas. We also integrate a large-scale pre-trained monocular depth estimator within the Gaussians optimization process, leveraging online augmented views to guide the geometric optimization towards an optimal solution. Starting from sparse points observed from limited input viewpoints, our FSGS can accurately grow into unseen regions, comprehensively covering the scene and boosting the rendering quality of novel views. Overall, FSGS achieves state-of-the-art performance in both accuracy and rendering efficiency across diverse datasets, including LLFF, Mip-NeRF360, and Blender.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2404.09465">PhyScene: Physically Interactable 3D Scene Synthesis for Embodied AI</a></div><div class="authors">Yang,  Yandan; Jia,  Baoxiong; Zhi,  Peiyuan; Huang,  Siyuan</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2404.09465" data-type="Paper">Paper</a> <a href="posters/poster_11.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">With recent developments in Embodied Artificial Intelligence (EAI) research, there has been a growing demand for high-quality, large-scale interactive scene generation. While prior methods in scene synthesis have prioritized the naturalness and realism of the generated scenes, the physical plausibility and interactivity of scenes have been largely left unexplored. To address this disparity, we introduce PhyScene, a novel method dedicated to generating interactive 3D scenes characterized by realistic layouts, articulated objects, and rich physical interactivity tailored for embodied agents. Based on a conditional diffusion model for capturing scene layouts, we devise novel physics- and interactivity-based guidance mechanisms that integrate constraints from object collision, room layout, and object reachability. Through extensive experiments, we demonstrate that PhyScene effectively leverages these guidance functions for physically interactable scene synthesis, outperforming existing state-of-the-art scene synthesis methods by a large margin. Our findings suggest that the scenes generated by PhyScene hold considerable potential for facilitating diverse skill acquisition among agents within interactive environments, thereby catalyzing further advancements in embodied AI research.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="papers/paper_21.pdf">Consistency^2: Consistent and Fast 3D Painting with Latent Consistency Models</a></div><div class="authors">Wang,  Tianfu; Obukhov,  Anton; Schindler,  Konrad</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="papers/paper_21.pdf" data-type="Paper">Paper</a> <a href="posters/poster_21.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Generative 3D Painting is among the top productivity boosters in high-resolution 3D asset management and recy- cling. Ever since text-to-image models became accessible for inference on consumer hardware, the performance of 3D Painting methods has consistently improved and is cur- rently close to plateauing. At the core of most such models lies denoising diffusion in the latent space, an inherently time-consuming iterative process. Multiple techniques have been developed recently to accelerate generation and reduce sampling iterations by orders of magnitude. Designed for 2D generative imaging, these techniques do not come with recipes for lifting them into 3D. In this paper, we ad- dress this shortcoming by proposing a Latent Consistency Model (LCM) adaptation for the task at hand. We analyze the strengths and weaknesses of the proposed model and evaluate it quantitatively and qualitatively. Based on the Objaverse dataset samples study, our 3D painting method attains strong preference in all evaluations.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/pdf/2312.05208">ControlRoom3D: Room Generation using Semantic Proxy Rooms</a></div><div class="authors">Schult,  Jonas; Tsai,  Sam; Höllein,  Lukas; Wu,  Bichen; Wang,  Jialiang; Ma,  Chih-Yao; Li,  Kunpeng Optional; Wang,  Xiaofang; Wimbauer,  Felix; He,  Zijian; Zhang,  Peizhao; Leibe,  Bastian; Vajda,  Peter; Hou,  Ji</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2312.05208" data-type="Paper">Paper</a> <a href="posters/poster_7.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Manually creating 3D environments for AR/VR applications is a complex process requiring expert knowledge in 3D modeling software. Pioneering works facilitate this process by generating room meshes conditioned on textual style descriptions. Yet, many of these automatically generated 3D meshes do not adhere to typical room layouts, compromising their plausibility, e.g., by placing several beds in one bedroom. To address these challenges, we present ControlRoom3D, a novel method to generate high-quality room meshes. Central to our approach is a user-defined 3D semantic proxy room that outlines a rough room layout based on semantic bounding boxes and a textual description of the overall room style. Our key insight is that when rendered to 2D, this 3D representation provides valuable geometric and semantic information to control powerful 2D models to generate 3D consistent textures and geometry that aligns well with the proxy room. Backed up by an extensive study including quantitative metrics and qualitative user evaluations, our method generates diverse and globally plausible 3D room meshes, thus empowering users to design 3D rooms effortlessly without specialized knowledge.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2404.10279">EucliDreamer: Fast and High-Quality Texturing for 3D Models with Depth-Conditioned Stable Diffusion</a></div><div class="authors">Le,  Cindy; Hetang,  Congrui; Lin,  Chendi; Cao,  Ang ; He,  Yihui</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2404.10279" data-type="Paper">Paper</a> <a href="posters/poster_28.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">We present EucliDreamer, a simple and effective method to generate textures for 3D models given text prompts and meshes. The texture is parametrized as an implicit function on the 3D surface, which is optimized with the Score Distillation Sampling (SDS) process and differentiable rendering. To generate high-quality textures, we leverage a depth-conditioned Stable Diffusion model guided by the depth image rendered from the mesh. We test our approach on 3D models in Objaverse and conducted a user study, which shows its superior quality compared to existing texturing methods like Text2Tex. In addition, our method converges 2 times faster than DreamFusion. Through text prompting, textures of diverse art styles can be produced. We hope Euclidreamer proides a viable solution to automate a labor-intensive stage in 3D content creation.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/pdf/2405.10508">ART3D: 3D Gaussian Splatting for Text-Guided Artistic Scenes Generation</a></div><div class="authors">Li,  Pengzhi; Tang,  Chengshuai; Huang,  Qinxuan; Li,  Zhiheng</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2405.10508" data-type="Paper">Paper</a> <a href="posters/poster_34.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">In this paper, we explore the existing challenges in 3D artistic scene generation by introducing ART3D, a novel framework that combines diffusion models and 3D Gaussian splatting techniques. Our method effectively bridges the gap between artistic and realistic images through an innovative image semantic transfer algorithm. By leveraging depth information and an initial artistic image, we generate a point cloud map, addressing domain differences. Additionally, we propose a depth consistency module to enhance 3D scene consistency. Finally, the 3D scene serves as initial points for optimizing Gaussian splats. Experimental results demonstrate ART3D's superior performance in both content and structural consistency metrics when compared to existing methods. ART3D significantly advances the field of AI in art creation by providing an innovative solution for generating high-quality 3D artistic scenes.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="">Isometric View Images Generation from Three Orthographic View Contour Drawings using Enhanced IsoGAN</a></div><div class="authors">Nguyen,  Thao Phuong*; Sakaino,  Hidetomo</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="" data-type="Paper">Paper</a> <a href="posters/poster_31.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Reconstructing the 2D or 3D shape of an object from several 2D drawings is a crucial problem in computer-aided design (CAD). Despite the advancement of deep neural networks, automatic isometric image generation from three orthographic views line drawings using deep learning remains unresolved. Existing image-to-image translation techniques often generate images from just one input image. In this paper, we propose a novel method for the above task using a GAN-based model, namely IsoGAN. This method takes three images of object's front, side, and top view as input, then analyzes the spatial and geometrical relations between each view and finally generates the corresponding isometric view image of the object. Extensive experiments on SPARE3D dataset show promising results of IsoGAN on isometric view generation task, demonstrating the effectiveness of the proposed IsoGAN.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Bhattad_StyLitGAN_Image-Based_Relighting_via_Latent_Control_CVPR_2024_paper.pdf">StyLitGAN: Image-based Relighting via Latent Control</a></div><div class="authors">Bhattad,  Anand; Soole,  James; Forsyth,  David</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Bhattad_StyLitGAN_Image-Based_Relighting_via_Latent_Control_CVPR_2024_paper.pdf" data-type="Paper">Paper</a><div class="link-content" data-index="0">We describe a novel method, StyLitGAN, for relighting and resurfacing images in the absence of labeled data. StyLitGAN generates images with realistic lighting effects, including cast shadows, soft shadows, inter-reflections, and glossy effects, without the need for paired or CGI data. StyLitGAN uses an intrinsic image method to decompose an image, followed by a search of the latent space of a pretrained StyleGAN to identify a set of directions. By prompting the model to fix one component (e.g., albedo) and vary another (e.g., shading), we generate relighted images by adding the identified directions to the latent style codes. Quantitative metrics of change in albedo and lighting diversity allow us to choose effective directions using a forward selection process. Qualitative evaluation confirms the effectiveness of our method.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="papers/paper_22.pdf">Single-Image Coherent Reconstruction of Objects and Humans</a></div><div class="authors">Batra,  Sarthak*; Chakrabarti,  Partha P; Hadfield,  Simon; Mustafa,  Armin</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="papers/paper_22.pdf" data-type="Paper">Paper</a> <a href="posters/poster_22.pptx" data-type="Poster">Poster</a><div class="link-content" data-index="0">Existing methods for reconstructing objects and humans from a monocular image suffer from severe mesh collisions and performance limitations for interacting occluding objects. This paper introduces a method to obtain a globally consistent 3D reconstruction of interacting objects and people from a single image. Our contributions include: 1) an optimization framework, featuring a collision loss, tailored to handle human-object and human-human interactions, ensuring spatially coherent scene reconstruction; and 2) a novel technique to robustly estimate 6 degrees of freedom (DOF) poses, specifically for heavily occluded objects, exploiting image inpainting. Notably, our proposed method operates effectively on images from real-world scenarios, without necessitating scene or object-level 3D supervision. Extensive qualitative and quantitative evaluation against existing methods demonstrates a significant reduction in collisions in the final reconstructions of scenes with multiple interacting humans and objects and a more coherent scene reconstruction.</div></div></div></div><!-- end paper list -->

        <div class="d-flex" id="footer">
            <div class="col-6 d-none d-md-block col-button" style="text-align: left;">
            </div>
            <div class="col-6 d-none d-md-block col-button" style="text-align: right;">
                <a class="btn btn-primary me-md-2" role="button" href="#home" >Top</a>
            </div>
        </div>
        <!---
        <div class="d-grid gap-2 d-md-flex justify-content-md-end d-none d-md-block col-button">
            <a class="btn btn-primary me-md-2" role="button" href="#home" >Top</a>
        </div>-->

        <script>
            var scrollSpy = new bootstrap.ScrollSpy(document.body, {
                target: '#navbar-top'
            });
        </script>
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-4LXC9BQBBX"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
        
          gtag('config', 'G-4LXC9BQBBX');
        </script>

        <!-- Javascript for showing and hiding the abstract and bibtex -->
        <script type="text/javascript">
            document.querySelectorAll(".links").forEach(function (p) {
                p.addEventListener("click", function (ev) {
                    // Make sure that the click is coming from a link
                    if (ev.target.nodeName != "A") {
                        return;
                    }
                    console.log(ev);

                    // Find the index of the div to toggle or return
                    var i = ev.target.dataset["index"];
                    if (i == undefined) {
                        return;
                    }

                    // Make sure to remove something else that was displayed
                    // and toggle the current one
                    Array.prototype.forEach.call(
                        ev.target.parentNode.children,
                        function (sibling) {
                            // We don't care about links etc
                            if (sibling.nodeName != "DIV") {
                                return;
                            }

                            // Hide others
                            if (sibling.dataset["index"] != i) {
                                sibling.style.display = "none";
                            }

                            // toggle the correct one
                            else {
                                if (sibling.style.display != "block") {
                                    sibling.style.display = "block";
                                } else {
                                    sibling.style.display = "none";
                                }
                            }
                        }
                    );
                    ev.preventDefault();
                });
            });
        </script>
    </body>
</html>
