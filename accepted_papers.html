<!Doctype html>
<html lang="en">
    <head>
        <title>AI3DG: Workshop on AI for 3D Generation</title>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="author" content="Despoina Paschalidou">
        <meta name="description" content="Workshop page for ai3dcc">
        <!-- Bootstrap -->
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" type="text/css" href="style.css?cache=77333914184988801948">
        <link rel="icon" type="image/png" href="figures/favicon.png"/>
    </head>
    <body>
        <div id="navbar-top">
        <nav class="navbar navbar-light px-3">
            <a class="navbar-brand" href="https://cvpr.thecvf.com"/>AI3DG @ CVPR2024</a>
            <ul class="nav nav-pills">
              <li class="nav-item">
                <a class="nav-link active" href="index.html#home">Home</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="index.html#submission">Submission</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="index.html#schedule">Schedule</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="accepted_papers.html">
                Accepted Papers</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="index.html#speakers">Speakers</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="program_committee.html">
                Program Committee</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="index.html#organizers">Organizers</a>
              </li>
            </ul>
        </nav>
        </div>
        <div data-bs-spy="scroll" data-bs-target="#navbar-example2" data-bs-offset="0" class="scrollspy-example" tabindex="0">

        <div class="jumbotron">
            <div class="papers-title">
                <h2>Workshop Publications</h2>
            </div>
            <!-- start paper list --><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2305.15094">InNeRF360: Text-Guided 3D-Consistent Object Inpainting on 360-degree Neural Radiance Fields</a></div><div class="authors">Wang,  Dongqing; Zhang,  Tong; Abboud,  Alaa; Süsstrunk,  Sabine</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2305.15094" data-type="Paper">Paper</a> <a href="posters/poster_19.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">We propose InNeRF360, an automatic system that accurately removes text-specified objects from 360-degree Neural Radiance Fields (NeRF). The challenge is to effectively remove objects while inpainting perceptually consistent content for the missing regions, which is particularly demanding for existing NeRF models due to their implicit volumetric representation. Moreover, unbounded scenes are more prone to floater artifacts in the inpainted region than frontal-facing scenes, as the change of object appearance and background across views is more sensitive to inaccurate segmentations and inconsistent inpainting. With a trained NeRF and a text description, our method efficiently removes specified objects and inpaints visually consistent content without artifacts. We apply depth-space warping to enforce consistency across multiview text-encoded segmentations, and then refine the inpainted NeRF model using perceptual priors and 3D diffusion-based geometric priors to ensure visual plausibility. Through extensive experiments in segmentation and inpainting on 360-degree and frontal-facing NeRFs, we show that our approach is effective and enhances NeRF's editability.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/pdf/2406.01136">Towards Practical Single-shot Motion Synthesis</a></div><div class="authors">Roditakis,  Konstantinos; Thermos,  Spyridon; Zioulis Nikolaos</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="papers/paper_15.pdf" data-type="Paper">Paper</a> <a href="posters/poster_15.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Despite the recent advances in the so-called 'cold start' generation from text prompts, their needs in data and computing resources, as well as the ambiguities around intellectual property and privacy concerns pose certain counterarguments for their utility. An interesting and relatively unexplored alternative has been the introduction of unconditional synthesis from a single sample, which has led to interesting generative applications. In this paper we focus on single-shot motion generation and more specifically on accelerating the training time of a Generative Adversarial Network (GAN). In particular, we tackle the challenge of GAN's equilibrium collapse when using mini-batch training by carefully annealing the weights of the loss functions that prevent mode collapse. Additionally, we perform statistical analysis in the generator and discriminator models to identify correlations between training stages and enable transfer learning. Our improved GAN achieves competitive quality and diversity on the Mixamo benchmark when compared to the original GAN architecture and a single-shot diffusion model, while being up to x6.8 faster in training time from the former and x1.75 from the latter. Finally, we demonstrate the ability of our improved GAN to mix and compose motion with a single forward pass.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://showlab.github.io/DynVideo-E/">DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing</a></div><div class="authors">Liu,  Jia-Wei; Cao,  Yan-Pei; Wu,  Jay Zhangjie; Mao,  Weijia; Gu,  Yuchao; Zhao,  Rui; Keppo,  Jussi; Shan,  Ying; Shou,  Mike Zheng</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2310.10624" data-type="Paper">Paper</a><div class="link-content" data-index="0">Despite recent progress in diffusion-based video editing, existing methods are limited to short-length videos due to the contradiction between long-range consistency and frame-wise editing. Prior attempts to address this challenge by introducing video-2D representations encounter significant difficulties with large-scale motion- and view-change videos, especially in human-centric scenarios. To overcome this, we propose to introduce the dynamic Neural Radiance Fields (NeRF) as the innovative video representation, where the editing can be performed in the 3D spaces and propagated to the entire video via the deformation field. To provide consistent and controllable editing, we propose the image-based video-NeRF editing pipeline with a set of innovative designs, including multi-view multi-pose Score Distillation Sampling (SDS) from both the 2D personalized diffusion prior and 3D diffusion prior, reconstruction losses, text-guided local parts super-resolution, and style transfer. Extensive experiments demonstrate that our method, dubbed as DynVideo-E, significantly outperforms SOTA approaches on two challenging datasets by a large margin of 50% ~ 95% for human preference.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="papers/paper_6.pdf">Intrinsic LoRA: A Generalist Approach for Discovering Knowledge in Generative Models</a></div><div class="authors">Du,  Xiaodan; Kolkin,  Nicholas I; Shakhnarovich,  Greg; Bhattad,  Anand</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="papers/paper_6.pdf" data-type="Paper">Paper</a> <a href="posters/poster_6.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Generative models have been shown to be capable of creating images that closely mimic real scenes, suggesting they inherently encode scene representations. We introduce INTRINSIC LORA (I-LORA), a general approach that uses Low-Rank Adaptation (LoRA) to discover scene intrinsics such as normals, depth, albedo, and shading from a wide array of generative models. I-LORA is lightweight, adding minimally to the model’s parameters and requiring very small datasets for this knowledge discovery. Our approach, applicable to Diffusion models, GANs, and Autoregressive models alike, generates intrinsics using the same output head as the original images.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="http://raywzy.com/CAD/">CAD: Photorealistic 3D Generation via Adversarial Distillation</a></div><div class="authors">Wan,  Ziyu; Paschalidou,  Despoina; Huang,  Ian Y; Liu,  Hongyu; Shen,  Bokui; Xiang,  Xiaoyu; Liao,  Jing; Guibas,  Leonidas</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2312.06663" data-type="Paper">Paper</a><div class="link-content" data-index="0">The increased demand for 3D data in AR/VR, robotics and gaming applications, gave rise to powerful generative pipelines capable of synthesizing high-quality 3D objects. Most of these models rely on the Score Distillation Sampling (SDS) algorithm to optimize a 3D representation such that the rendered image maintains a high likelihood as evaluated by a pre-trained diffusion model. However, finding a correct mode in the high-dimensional distribution produced by the diffusion model is challenging and often leads to issues such as over-saturation, over-smoothing, and Janus-like artifacts. In this paper, we propose a novel learning paradigm for 3D synthesis that utilizes pre-trained diffusion models. Instead of focusing on mode-seeking, our method directly models the distribution discrepancy between multi-view renderings and diffusion priors in an adversarial manner, which unlocks the generation of high-fidelity and photorealistic 3D content, conditioned on a single image and prompt. Moreover, by harnessing the latent space of GANs and expressive diffusion model priors, our method facilitates a wide variety of 3D applications including single-view reconstruction, high diversity generation and continuous 3D interpolation in the open domain. The experiments demonstrate the superiority of our pipeline compared to previous works in terms of generation quality and diversity.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://threedle.github.io/3d-paintbrush/">3D Paintbrush: Local Stylization of 3D Shapes with Cascaded Score Distillation</a></div><div class="authors">Decatur,  Dale; Lang,  Itai; Aberman,  Kfir; Hanocka,  Rana</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2311.09571" data-type="Paper">Paper</a><div class="link-content" data-index="0">In this work we develop 3D Paintbrush, a technique for automatically texturing local semantic regions on meshes via text descriptions. Our method is designed to operate directly on meshes, producing texture maps which seamlessly integrate into standard graphics pipelines. We opt to simultaneously produce a localization map (to specify the edit region) and a texture map which conforms to it. This synergistic approach improves the quality of both the localization and the stylization. To enhance the details and resolution of the textured area, we leverage multiple stages of a cascaded diffusion model to supervise our local editing technique with generative priors learned from images at different resolutions. Our technique, referred to as Cascaded Score Distillation (CSD), simultaneously distills scores at multiple resolutions in a cascaded fashion, enabling control over both the granularity and global understanding of the supervision. We demonstrate the effectiveness of 3D Paintbrush to locally texture a variety of shapes within different semantic regions.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2404.06903">DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting</a></div><div class="authors">Zhou,  Shijie; Fan,  Zhiwen; Xu,  Dejia; Chang,  Haoran; Chari,  Pradyumna; Bharadwaj,  Tejas K; You,  Suya; Wang,  Zhangyang; Kadambi,  Achuta</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2404.06903" data-type="Paper">Paper</a><div class="link-content" data-index="0">The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360∘ scene generation pipeline that facilitates the creation of comprehensive 360∘ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary 'flat' (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360∘ perspective, providing an enhanced immersive experience over existing techniques.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2406.06133">ExtraNeRF: Visibility-Aware View Extrapolation of Neural Radiance Fields with Diffusion Models</a></div><div class="authors">Shih,  Meng-Li; Ma,  Wei-Chiu; Boyice,  Lorenzo; Holynski,  Aleksander; Cole,  Forrester; Kontkanen,  Janne; Curless,  Brian</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2406.06133" data-type="Paper">Paper</a><div class="link-content" data-index="0">We propose ExtraNeRF, a novel method for extrapolating the range of views handled by a Neural Radiance Field (NeRF). Our main idea is to leverage NeRFs to model scene-specific, fine-grained details, while capitalizing on diffusion models to extrapolate beyond our observed data. A key ingredient is to track visibility to determine what portions of the scene have not been observed, and focus on reconstructing those regions consistently with diffusion models. Our primary contributions include a visibility-aware diffusion-based inpainting module that is fine-tuned on the input imagery, yielding an initial NeRF with moderate quality (often blurry) inpainted regions, followed by a second diffusion model trained on the input imagery to consistently enhance, notably sharpen, the inpainted imagery from the first pass. We demonstrate high-quality results, extrapolating beyond a small number of (typically six or fewer) input views, effectively outpainting the NeRF as well as inpainting newly disoccluded regions inside the original viewing volume. We compare with related work both quantitatively and qualitatively and show significant gains over prior art.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2405.09050v1">3D Shape Augmentation with Content-Aware Shape Resizing</a></div><div class="authors">Chen,  Mingxiang; Zhou,  Boli; Zhang,  Jian</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2405.09050v1" data-type="Paper">Paper</a><div class="link-content" data-index="0">Recent advancements in deep learning for 3D models have propelled breakthroughs in generation, detection, and scene understanding. However, the effectiveness of these algorithms hinges on large training datasets. We address the challenge by introducing Efficient 3D Seam Carving (E3SC), a novel 3D model augmentation method based on seam carving, which progressively deforms only part of the input model while ensuring the overall semantics are unchanged. Experiments show that our approach is capable of producing diverse and high-quality augmented 3D shapes across various types and styles of input models, achieving considerable improvements over previous methods. Quantitative evaluations demonstrate that our method effectively enhances the novelty and quality of shapes generated by other subsequent 3D generation algorithms.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2402.15509">Seamless Human Motion Composition with Blended Positional Encodings</a></div><div class="authors">Barquero,  German; Escalera,  Sergio; Palmero,  Cristina</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2402.15509" data-type="Paper">Paper</a><div class="link-content" data-index="0">Conditional human motion generation is an important topic with many applications in virtual reality, gaming, and robotics. While prior works have focused on generating motion guided by text, music, or scenes, these typically result in isolated motions confined to short durations. Instead, we address the generation of long, continuous sequences guided by a series of varying textual descriptions. In this context, we introduce FlowMDM, the first diffusion-based model that generates seamless Human Motion Compositions (HMC) without any postprocessing or redundant denoising steps. For this, we introduce the Blended Positional Encodings, a technique that leverages both absolute and relative positional encodings in the denoising chain. More specifically, global motion coherence is recovered at the absolute stage, whereas smooth and realistic transitions are built at the relative stage. As a result, we achieve state-of-the-art results in terms of accuracy, realism, and smoothness on the Babel and HumanML3D datasets. FlowMDM excels when trained with only a single description per motion sequence thanks to its Pose-Centric Cross-ATtention, which makes it robust against varying text descriptions at inference time. Finally, to address the limitations of existing HMC metrics, we propose two new metrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt transitions.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2406.07742">C3DAG: Controlled 3D Animal Generation using 3D pose guidance</a></div><div class="authors">Mishra,  Sandeep; Saha,  Oindrila ; Bovik,  Alan</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2406.07742" data-type="Paper">Paper</a> <a href="posters/poster_27.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Recent advancements in text-to-3D generation have demonstrated the ability to generate high quality 3D assets. However while generating animals these methods underperform, often portraying inaccurate anatomy and geometry. Towards ameliorating this defect, we present C3DAG, a novel pose-Controlled text-to-3D Animal Generation framework which generates a high quality 3D animal consistent with a given pose. We also introduce an automatic 3D shape creator tool, that allows dynamic pose generation and modification via a web-based tool, and that generates a 3D balloon animal using simple geometries. A NeRF is then initialized using this 3D shape using depth-controlled SDS. In the next stage, the pre-trained NeRF is fine-tuned using quadruped-pose-controlled SDS. The pipeline that we have developed not only produces geometrically and anatomically consistent results, but also renders highly controlled 3D animals, unlike prior methods which do not allow fine-grained pose control.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2312.00451">FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</a></div><div class="authors">Zhu,  Zehao; Fan,  Zhiwen; Jiang,  Yifan; Cong,  Wenyan; Wang,  Zhangyang</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2312.00451" data-type="Paper">Paper</a> <a href="posters/poster_41.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Novel view synthesis from limited observations remains an important and persistent task. However, high efficiency in existing NeRF-based few-shot view synthesis is often compromised to obtain an accurate 3D representation. To address this challenge, we propose a few-shot view synthesis framework based on 3D Gaussian Splatting that enables real-time and photo-realistic view synthesis with as few as three training views. The proposed method, dubbed FSGS, handles the extremely sparse initialized SfM points with a thoughtfully designed Gaussian Unpooling process. Our method iteratively distributes new Gaussians around the most representative locations, subsequently infilling local details in vacant areas. We also integrate a large-scale pre-trained monocular depth estimator within the Gaussians optimization process, leveraging online augmented views to guide the geometric optimization towards an optimal solution. Starting from sparse points observed from limited input viewpoints, our FSGS can accurately grow into unseen regions, comprehensively covering the scene and boosting the rendering quality of novel views. Overall, FSGS achieves state-of-the-art performance in both accuracy and rendering efficiency across diverse datasets, including LLFF, Mip-NeRF360, and Blender.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://sherwinbahmani.github.io/4dfy/">4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling</a></div><div class="authors">Bahmani,  Sherwin; Skorokhodov,  Ivan; Rong,  Victor; Wetzstein,  Gordon; Guibas,  Leonidas; Wonka,  Peter; Tulyakov,  Sergey; Park,  Jeong Joon; Tagliasacchi,  Andrea; Lindell,  David B</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2311.17984" data-type="Paper">Paper</a> <a href="posters/poster_36.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Recent breakthroughs in text-to-4D generation rely on pre-trained text-to-image and text-to-video models to generate dynamic 3D scenes. However, current text-to-4D methods face a three-way tradeoff between the quality of scene appearance, 3D structure, and motion. For example, text-to-image models and their 3D-aware variants are trained on internet-scale image datasets and can be used to produce scenes with realistic appearance and 3D structure -- but no motion. Text-to-video models are trained on relatively smaller video datasets and can produce scenes with motion, but poorer appearance and 3D structure. While these models have complementary strengths, they also have opposing weaknesses, making it difficult to combine them in a way that alleviates this three-way tradeoff. Here, we introduce hybrid score distillation sampling, an alternating optimization procedure that blends supervision signals from multiple pre-trained diffusion models and incorporates benefits of each for high-fidelity text-to-4D generation. Using hybrid SDS, we demonstrate synthesis of 4D scenes with compelling appearance, 3D structure, and motion.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2404.09465">PhyScene: Physically Interactable 3D Scene Synthesis for Embodied AI</a></div><div class="authors">Yang,  Yandan; Jia,  Baoxiong; Zhi,  Peiyuan; Huang,  Siyuan</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2404.09465" data-type="Paper">Paper</a> <a href="posters/poster_11.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">With recent developments in Embodied Artificial Intelligence (EAI) research, there has been a growing demand for high-quality, large-scale interactive scene generation. While prior methods in scene synthesis have prioritized the naturalness and realism of the generated scenes, the physical plausibility and interactivity of scenes have been largely left unexplored. To address this disparity, we introduce PhyScene, a novel method dedicated to generating interactive 3D scenes characterized by realistic layouts, articulated objects, and rich physical interactivity tailored for embodied agents. Based on a conditional diffusion model for capturing scene layouts, we devise novel physics- and interactivity-based guidance mechanisms that integrate constraints from object collision, room layout, and object reachability. Through extensive experiments, we demonstrate that PhyScene effectively leverages these guidance functions for physically interactable scene synthesis, outperforming existing state-of-the-art scene synthesis methods by a large margin. Our findings suggest that the scenes generated by PhyScene hold considerable potential for facilitating diverse skill acquisition among agents within interactive environments, thereby catalyzing further advancements in embodied AI research.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="papers/paper_21.pdf">Consistency^2: Consistent and Fast 3D Painting with Latent Consistency Models</a></div><div class="authors">Wang,  Tianfu; Obukhov,  Anton; Schindler,  Konrad</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="papers/paper_21.pdf" data-type="Paper">Paper</a> <a href="posters/poster_21.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Generative 3D Painting is among the top productivity boosters in high-resolution 3D asset management and recy- cling. Ever since text-to-image models became accessible for inference on consumer hardware, the performance of 3D Painting methods has consistently improved and is cur- rently close to plateauing. At the core of most such models lies denoising diffusion in the latent space, an inherently time-consuming iterative process. Multiple techniques have been developed recently to accelerate generation and reduce sampling iterations by orders of magnitude. Designed for 2D generative imaging, these techniques do not come with recipes for lifting them into 3D. In this paper, we ad- dress this shortcoming by proposing a Latent Consistency Model (LCM) adaptation for the task at hand. We analyze the strengths and weaknesses of the proposed model and evaluate it quantitatively and qualitatively. Based on the Objaverse dataset samples study, our 3D painting method attains strong preference in all evaluations.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/pdf/2312.05208">ControlRoom3D: Room Generation using Semantic Proxy Rooms</a></div><div class="authors">Schult,  Jonas; Tsai,  Sam; Höllein,  Lukas; Wu,  Bichen; Wang,  Jialiang; Ma,  Chih-Yao; Li,  Kunpeng Optional; Wang,  Xiaofang; Wimbauer,  Felix; He,  Zijian; Zhang,  Peizhao; Leibe,  Bastian; Vajda,  Peter; Hou,  Ji</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2312.05208" data-type="Paper">Paper</a> <a href="posters/poster_7.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Manually creating 3D environments for AR/VR applications is a complex process requiring expert knowledge in 3D modeling software. Pioneering works facilitate this process by generating room meshes conditioned on textual style descriptions. Yet, many of these automatically generated 3D meshes do not adhere to typical room layouts, compromising their plausibility, e.g., by placing several beds in one bedroom. To address these challenges, we present ControlRoom3D, a novel method to generate high-quality room meshes. Central to our approach is a user-defined 3D semantic proxy room that outlines a rough room layout based on semantic bounding boxes and a textual description of the overall room style. Our key insight is that when rendered to 2D, this 3D representation provides valuable geometric and semantic information to control powerful 2D models to generate 3D consistent textures and geometry that aligns well with the proxy room. Backed up by an extensive study including quantitative metrics and qualitative user evaluations, our method generates diverse and globally plausible 3D room meshes, thus empowering users to design 3D rooms effortlessly without specialized knowledge.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2312.11360">Paint-it: Text-to-Texture Synthesis via Deep Convolutional Texture Map Optimization and Physically-Based Rendering</a></div><div class="authors">Youwang,  Kim; Oh,  Tae-Hyun; Pons-Moll,  Gerard</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2312.11360" data-type="Paper">Paper</a> <a href="https://kim-youwang.github.io/media/paint-it/cvpr24_poster_youwang_final.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">We present Paint-it, a text-driven high-fidelity texture map synthesis method for 3D meshes via neural re-parameterized texture optimization. Paint-it synthesizes texture maps from a text description by synthesis-through-optimization, exploiting the Score-Distillation Sampling (SDS). We observe that directly applying SDS yields undesirable texture quality due to its noisy gradients. We reveal the importance of texture parameterization when using SDS. Specifically, we propose Deep Convolutional Physically-Based Rendering (DC-PBR) parameterization, which re-parameterizes the physically-based rendering (PBR) texture maps with randomly initialized convolution-based neural kernels, instead of a standard pixel-based parameterization. We show that DC-PBR inherently schedules the optimization curriculum according to texture frequency and naturally filters out the noisy signals from SDS. In experiments, Paint-it obtains remarkable quality PBR texture maps within 15 min., given only a text description. We demonstrate the generalizability and practicality of Paint-it by synthesizing high-quality texture maps for large-scale mesh datasets and showing test-time applications such as relighting and material control using a popular graphics engine.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2404.10279">EucliDreamer: Fast and High-Quality Texturing for 3D Models with Depth-Conditioned Stable Diffusion</a></div><div class="authors">Le,  Cindy; Hetang,  Congrui; Lin,  Chendi; Cao,  Ang ; He,  Yihui</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2404.10279" data-type="Paper">Paper</a> <a href="posters/poster_28.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">We present EucliDreamer, a simple and effective method to generate textures for 3D models given text prompts and meshes. The texture is parametrized as an implicit function on the 3D surface, which is optimized with the Score Distillation Sampling (SDS) process and differentiable rendering. To generate high-quality textures, we leverage a depth-conditioned Stable Diffusion model guided by the depth image rendered from the mesh. We test our approach on 3D models in Objaverse and conducted a user study, which shows its superior quality compared to existing texturing methods like Text2Tex. In addition, our method converges 2 times faster than DreamFusion. Through text prompting, textures of diverse art styles can be produced. We hope Euclidreamer proides a viable solution to automate a labor-intensive stage in 3D content creation.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="">Volumetric Style Transfer using Neural Cellular Automata</a></div><div class="authors">Wang,  Dongqing; Pajouheshgar,  Ehsan; Xu,  Yitao; Zhang,  Tong; Süsstrunk,  Sabine</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="" data-type="Paper">Paper</a> <a href="posters/poster_20.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Artistic stylization of 3D volumetric smoke data is still a challenge in computer graphics due to the difficulty of ensuring spatiotemporal consistency given a reference style image, and that within reasonable time and computational resources. In this work, we introduce Volumetric Neural Cellular Automata (VNCA), a novel model for efficient volumetric style transfer that synthesizes, in real-time, multi-view consistent stylizing features on the target smoke with temporally coherent transitions between stylized simulation frames. VNCA synthesizes a 3D texture volume with color and density stylization and dynamically aligns this volume with the intricate motion patterns of the smoke simulation under the Eulerian framework. Our approach replaces the explicit fluid advection modeling and the inter-frame smoothing terms with the self-emerging motion of the underlying cellular automaton, thus reducing the training time by over an order of magnitude. Beyond smoke simulations, we demonstrate the versatility of our approach by showcasing its applicability to mesh stylization.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://as-plausible-as-possible.github.io/">As-Plausible-As-Possible: Plausibility-Aware Mesh Deformation Using 2D Diffusion Priors</a></div><div class="authors">Yoo,  Seungwoo; Kim,  Kunho; Kim,  Vladimir; Sung,  Minhyuk</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2311.16739" data-type="Paper">Paper</a><div class="link-content" data-index="0">We present As-Plausible-as-Possible (APAP) mesh deformation technique that leverages 2D diffusion priors to preserve the plausibility of a mesh under user-controlled deformation. Our framework uses per-face Jacobians to represent mesh deformations, where mesh vertex coordinates are computed via a differentiable Poisson Solve. The deformed mesh is rendered, and the resulting 2D image is used in the Score Distillation Sampling (SDS) process, which enables extracting meaningful plausibility priors from a pretrained 2D diffusion model. To better preserve the identity of the edited mesh, we fine-tune our 2D diffusion model with LoRA. Gradients extracted by SDS and a user-prescribed handle displacement are then backpropagated to the per-face Jacobians, and we use iterative gradient descent to compute the final deformation that balances between the user edit and the output plausibility. We evaluate our method with 2D and 3D meshes and demonstrate qualitative and quantitative improvements when using plausibility priors over geometry-preservation or distortion-minimization priors used by previous techniques</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="papers/paper_43.pdf">An Ethical Framework for Trustworthy Neural Rendering applied in Cultural Heritage and Creative Industries</a></div><div class="authors">Stacchio,  Lorenzo; Balloni,  Emanuele; Gorgoglione,  Lucrezia; Pierdicca,  Roberto; Mancini,  Adriano; Frontoni,  Emanuele; Giovanola,  Benedetta; Tiribelli,  Simona; Paolanti,  Marina; Zingaretti,  Primo</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="papers/paper_43.pdf" data-type="Paper">Paper</a> <a href="posters/poster_43.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Artificial Intelligence (AI) has revolutionized various sectors, including Cultural Heritage (CH) and Creative Industries (CI), defining novel opportunities and challenges in preserving tangible and intangible human productions. In such a context, Neural Rendering (NR) paradigms play the pivotal role of 3D reconstructing objects or scenes by optimizing images depicting them. However, there is a lack of work examining the ethical concerns associated with its usage. Those are particularly relevant in scenarios where NR is applied to items protected by intellectual property rights, UNESCO-recognised heritage sites, or items critical for data-driven decisions. For this, we here outline the main ethical findings in this area and place them in a novel framework to guide stakeholders and developers through principles and risks associated with the use of NR in CH and CI. Such a framework examines AI’s ethical principles supporting the definition of novel ethical guidelines.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/pdf/2405.10508">ART3D: 3D Gaussian Splatting for Text-Guided Artistic Scenes Generation</a></div><div class="authors">Li,  Pengzhi; Tang,  Chengshuai; Huang,  Qinxuan; Li,  Zhiheng</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2405.10508" data-type="Paper">Paper</a> <a href="posters/poster_34.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">In this paper, we explore the existing challenges in 3D artistic scene generation by introducing ART3D, a novel framework that combines diffusion models and 3D Gaussian splatting techniques. Our method effectively bridges the gap between artistic and realistic images through an innovative image semantic transfer algorithm. By leveraging depth information and an initial artistic image, we generate a point cloud map, addressing domain differences. Additionally, we propose a depth consistency module to enhance 3D scene consistency. Finally, the 3D scene serves as initial points for optimizing Gaussian splats. Experimental results demonstrate ART3D's superior performance in both content and structural consistency metrics when compared to existing methods. ART3D significantly advances the field of AI in art creation by providing an innovative solution for generating high-quality 3D artistic scenes.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://posterior-distillation-sampling.github.io/">Posterior Distillation Sampling</a></div><div class="authors">Koo,  Juil; Park,  Chanho; Sung,  Minhyuk</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2311.13831" data-type="Paper">Paper</a><div class="link-content" data-index="0">We introduce Posterior Distillation Sampling (PDS), a novel optimization method for parametric image editing based on diffusion models. Existing optimization-based methods, which leverage the powerful 2D prior of diffusion models to handle various parametric images, have mainly focused on generation. Unlike generation, editing requires a balance between conforming to the target attribute and preserving the identity of the source content. Recent 2D image editing methods have achieved this balance by leveraging the stochastic latent encoded in the generative process of diffusion models. To extend the editing capabilities of diffusion models shown in pixel space to parameter space, we reformulate the 2D image editing method into an optimization form named PDS. PDS matches the stochastic latents of the source and the target, enabling the sampling of targets in diverse parameter spaces that align with a desired attribute while maintaining the source's identity. We demonstrate that this optimization resembles running a generative process with the target attribute, but aligning this process with the trajectory of the source's generative process. Extensive editing results in Neural Radiance Fields and Scalable Vector Graphics representations demonstrate that PDS is capable of sampling targets to fulfill the aforementioned balance across various parameter spaces.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="">Isometric View Images Generation from Three Orthographic View Contour Drawings using Enhanced IsoGAN</a></div><div class="authors">Nguyen,  Thao Phuong*; Sakaino,  Hidetomo</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="" data-type="Paper">Paper</a> <a href="posters/poster_31.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Reconstructing the 2D or 3D shape of an object from several 2D drawings is a crucial problem in computer-aided design (CAD). Despite the advancement of deep neural networks, automatic isometric image generation from three orthographic views line drawings using deep learning remains unresolved. Existing image-to-image translation techniques often generate images from just one input image. In this paper, we propose a novel method for the above task using a GAN-based model, namely IsoGAN. This method takes three images of object's front, side, and top view as input, then analyzes the spatial and geometrical relations between each view and finally generates the corresponding isometric view image of the object. Extensive experiments on SPARE3D dataset show promising results of IsoGAN on isometric view generation task, demonstrating the effectiveness of the proposed IsoGAN.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Bhattad_StyLitGAN_Image-Based_Relighting_via_Latent_Control_CVPR_2024_paper.pdf">StyLitGAN: Image-based Relighting via Latent Control</a></div><div class="authors">Bhattad,  Anand; Soole,  James; Forsyth,  David</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Bhattad_StyLitGAN_Image-Based_Relighting_via_Latent_Control_CVPR_2024_paper.pdf" data-type="Paper">Paper</a><div class="link-content" data-index="0">We describe a novel method, StyLitGAN, for relighting and resurfacing images in the absence of labeled data. StyLitGAN generates images with realistic lighting effects, including cast shadows, soft shadows, inter-reflections, and glossy effects, without the need for paired or CGI data. StyLitGAN uses an intrinsic image method to decompose an image, followed by a search of the latent space of a pretrained StyleGAN to identify a set of directions. By prompting the model to fix one component (e.g., albedo) and vary another (e.g., shading), we generate relighted images by adding the identified directions to the latent style codes. Quantitative metrics of change in albedo and lighting diversity allow us to choose effective directions using a forward selection process. Qualitative evaluation confirms the effectiveness of our method.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="papers/paper_22.pdf">Single-Image Coherent Reconstruction of Objects and Humans</a></div><div class="authors">Batra,  Sarthak*; Chakrabarti,  Partha P; Hadfield,  Simon; Mustafa,  Armin</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="papers/paper_22.pdf" data-type="Paper">Paper</a> <a href="posters/poster_22.pptx" data-type="Poster">Poster</a><div class="link-content" data-index="0">Existing methods for reconstructing objects and humans from a monocular image suffer from severe mesh collisions and performance limitations for interacting occluding objects. This paper introduces a method to obtain a globally consistent 3D reconstruction of interacting objects and people from a single image. Our contributions include: 1) an optimization framework, featuring a collision loss, tailored to handle human-object and human-human interactions, ensuring spatially coherent scene reconstruction; and 2) a novel technique to robustly estimate 6 degrees of freedom (DOF) poses, specifically for heavily occluded objects, exploiting image inpainting. Notably, our proposed method operates effectively on images from real-world scenarios, without necessitating scene or object-level 3D supervision. Extensive qualitative and quantitative evaluation against existing methods demonstrates a significant reduction in collisions in the final reconstructions of scenes with multiple interacting humans and objects and a more coherent scene reconstruction.</div></div></div></div><!-- end paper list -->

        <div class="d-flex" id="footer">
            <div class="col-6 d-none d-md-block col-button" style="text-align: left;">
            </div>
            <div class="col-6 d-none d-md-block col-button" style="text-align: right;">
                <a class="btn btn-primary me-md-2" role="button" href="#home" >Top</a>
            </div>
        </div>
        <!---
        <div class="d-grid gap-2 d-md-flex justify-content-md-end d-none d-md-block col-button">
            <a class="btn btn-primary me-md-2" role="button" href="#home" >Top</a>
        </div>-->

        <script>
            var scrollSpy = new bootstrap.ScrollSpy(document.body, {
                target: '#navbar-top'
            });
        </script>
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-4LXC9BQBBX"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
        
          gtag('config', 'G-4LXC9BQBBX');
        </script>

        <!-- Javascript for showing and hiding the abstract and bibtex -->
        <script type="text/javascript">
            document.querySelectorAll(".links").forEach(function (p) {
                p.addEventListener("click", function (ev) {
                    // Make sure that the click is coming from a link
                    if (ev.target.nodeName != "A") {
                        return;
                    }
                    console.log(ev);

                    // Find the index of the div to toggle or return
                    var i = ev.target.dataset["index"];
                    if (i == undefined) {
                        return;
                    }

                    // Make sure to remove something else that was displayed
                    // and toggle the current one
                    Array.prototype.forEach.call(
                        ev.target.parentNode.children,
                        function (sibling) {
                            // We don't care about links etc
                            if (sibling.nodeName != "DIV") {
                                return;
                            }

                            // Hide others
                            if (sibling.dataset["index"] != i) {
                                sibling.style.display = "none";
                            }

                            // toggle the correct one
                            else {
                                if (sibling.style.display != "block") {
                                    sibling.style.display = "block";
                                } else {
                                    sibling.style.display = "none";
                                }
                            }
                        }
                    );
                    ev.preventDefault();
                });
            });
        </script>
    </body>
</html>
