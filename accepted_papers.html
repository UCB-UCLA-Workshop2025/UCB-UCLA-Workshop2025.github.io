<!Doctype html>
<html lang="en">
    <head>
        <title>AI3DG: Workshop on AI for 3D Generation</title>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="author" content="Despoina Paschalidou">
        <meta name="description" content="Workshop page for ai3dcc">
        <!-- Bootstrap -->
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" type="text/css" href="style.css?cache=77333914184988801948">
        <link rel="icon" type="image/png" href="figures/favicon.png"/>
    </head>
    <body>
        <div id="navbar-top">
        <nav class="navbar navbar-light px-3">
            <a class="navbar-brand" href="https://cvpr.thecvf.com"/>AI3DG @ CVPR2024</a>
            <ul class="nav nav-pills">
              <li class="nav-item">
                <a class="nav-link active" href="index.html#home">Home</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="index.html#submission">Submission</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="index.html#schedule">Schedule</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="accepted_papers.html">
                Accepted Papers</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="index.html#speakers">Speakers</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="program_committee.html">
                Program Committee</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="index.html#organizers">Organizers</a>
              </li>
            </ul>
        </nav>
        </div>
        <div data-bs-spy="scroll" data-bs-target="#navbar-example2" data-bs-offset="0" class="scrollspy-example" tabindex="0">

        <div class="jumbotron">
            <div class="papers-title">
                <h2>Workshop Publications</h2>
            </div>
            <!-- start paper list --><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/pdf/2406.01136">Towards Practical Single-shot Motion Synthesis</a></div><div class="authors">Roditakis,  Konstantinos; Thermos,  Spyridon; Zioulis Nikolaos</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="papers/paper_15.pdf" data-type="Paper">Paper</a> <a href="posters/poster_15.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Despite the recent advances in the so-called 'cold start' generation from text prompts, their needs in data and computing resources, as well as the ambiguities around intellectual property and privacy concerns pose certain counterarguments for their utility. An interesting and relatively unexplored alternative has been the introduction of unconditional synthesis from a single sample, which has led to interesting generative applications. In this paper we focus on single-shot motion generation and more specifically on accelerating the training time of a Generative Adversarial Network (GAN). In particular, we tackle the challenge of GAN's equilibrium collapse when using mini-batch training by carefully annealing the weights of the loss functions that prevent mode collapse. Additionally, we perform statistical analysis in the generator and discriminator models to identify correlations between training stages and enable transfer learning. Our improved GAN achieves competitive quality and diversity on the Mixamo benchmark when compared to the original GAN architecture and a single-shot diffusion model, while being up to x6.8 faster in training time from the former and x1.75 from the latter. Finally, we demonstrate the ability of our improved GAN to mix and compose motion with a single forward pass.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2312.00451">FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</a></div><div class="authors">Zhu,  Zehao; Fan,  Zhiwen; Jiang,  Yifan; Cong,  Wenyan; Wang,  Zhangyang</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/abs/2312.00451" data-type="Paper">Paper</a> <a href="posters/poster_41.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Novel view synthesis from limited observations remains an important and persistent task. However, high efficiency in existing NeRF-based few-shot view synthesis is often compromised to obtain an accurate 3D representation. To address this challenge, we propose a few-shot view synthesis framework based on 3D Gaussian Splatting that enables real-time and photo-realistic view synthesis with as few as three training views. The proposed method, dubbed FSGS, handles the extremely sparse initialized SfM points with a thoughtfully designed Gaussian Unpooling process. Our method iteratively distributes new Gaussians around the most representative locations, subsequently infilling local details in vacant areas. We also integrate a large-scale pre-trained monocular depth estimator within the Gaussians optimization process, leveraging online augmented views to guide the geometric optimization towards an optimal solution. Starting from sparse points observed from limited input viewpoints, our FSGS can accurately grow into unseen regions, comprehensively covering the scene and boosting the rendering quality of novel views. Overall, FSGS achieves state-of-the-art performance in both accuracy and rendering efficiency across diverse datasets, including LLFF, Mip-NeRF360, and Blender.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="papers/paper_21.pdf">Consistency^2: Consistent and Fast 3D Painting with Latent Consistency Models</a></div><div class="authors">Wang,  Tianfu; Obukhov,  Anton; Schindler,  Konrad</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="papers/paper_21.pdf" data-type="Paper">Paper</a> <a href="posters/poster_21.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Generative 3D Painting is among the top productivity boosters in high-resolution 3D asset management and recy- cling. Ever since text-to-image models became accessible for inference on consumer hardware, the performance of 3D Painting methods has consistently improved and is cur- rently close to plateauing. At the core of most such models lies denoising diffusion in the latent space, an inherently time-consuming iterative process. Multiple techniques have been developed recently to accelerate generation and reduce sampling iterations by orders of magnitude. Designed for 2D generative imaging, these techniques do not come with recipes for lifting them into 3D. In this paper, we ad- dress this shortcoming by proposing a Latent Consistency Model (LCM) adaptation for the task at hand. We analyze the strengths and weaknesses of the proposed model and evaluate it quantitatively and qualitatively. Based on the Objaverse dataset samples study, our 3D painting method attains strong preference in all evaluations.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/pdf/2312.05208">ControlRoom3D: Room Generation using Semantic Proxy Rooms</a></div><div class="authors">Schult,  Jonas; Tsai,  Sam; HÃ¶llein,  Lukas; Wu,  Bichen; Wang,  Jialiang; Ma,  Chih-Yao; Li,  Kunpeng Optional; Wang,  Xiaofang; Wimbauer,  Felix; He,  Zijian; Zhang,  Peizhao; Leibe,  Bastian; Vajda,  Peter; Hou,  Ji</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2312.05208" data-type="Paper">Paper</a> <a href="posters/poster_7.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Manually creating 3D environments for AR/VR applications is a complex process requiring expert knowledge in 3D modeling software. Pioneering works facilitate this process by generating room meshes conditioned on textual style descriptions. Yet, many of these automatically generated 3D meshes do not adhere to typical room layouts, compromising their plausibility, e.g., by placing several beds in one bedroom. To address these challenges, we present ControlRoom3D, a novel method to generate high-quality room meshes. Central to our approach is a user-defined 3D semantic proxy room that outlines a rough room layout based on semantic bounding boxes and a textual description of the overall room style. Our key insight is that when rendered to 2D, this 3D representation provides valuable geometric and semantic information to control powerful 2D models to generate 3D consistent textures and geometry that aligns well with the proxy room. Backed up by an extensive study including quantitative metrics and qualitative user evaluations, our method generates diverse and globally plausible 3D room meshes, thus empowering users to design 3D rooms effortlessly without specialized knowledge.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Bhattad_StyLitGAN_Image-Based_Relighting_via_Latent_Control_CVPR_2024_paper.pdf">StyLitGAN: Image-based Relighting via Latent Control</a></div><div class="authors">Bhattad,  Anand; Soole,  James; Forsyth,  David</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Bhattad_StyLitGAN_Image-Based_Relighting_via_Latent_Control_CVPR_2024_paper.pdf" data-type="Paper">Paper</a><div class="link-content" data-index="0">We describe a novel method, StyLitGAN, for relighting and resurfacing images in the absence of labeled data. StyLitGAN generates images with realistic lighting effects, including cast shadows, soft shadows, inter-reflections, and glossy effects, without the need for paired or CGI data. StyLitGAN uses an intrinsic image method to decompose an image, followed by a search of the latent space of a pretrained StyleGAN to identify a set of directions. By prompting the model to fix one component (e.g., albedo) and vary another (e.g., shading), we generate relighted images by adding the identified directions to the latent style codes. Quantitative metrics of change in albedo and lighting diversity allow us to choose effective directions using a forward selection process. Qualitative evaluation confirms the effectiveness of our method.</div></div></div></div><!-- end paper list -->

        <div class="d-flex" id="footer">
            <div class="col-6 d-none d-md-block col-button" style="text-align: left;">
            </div>
            <div class="col-6 d-none d-md-block col-button" style="text-align: right;">
                <a class="btn btn-primary me-md-2" role="button" href="#home" >Top</a>
            </div>
        </div>
        <!---
        <div class="d-grid gap-2 d-md-flex justify-content-md-end d-none d-md-block col-button">
            <a class="btn btn-primary me-md-2" role="button" href="#home" >Top</a>
        </div>-->

        <script>
            var scrollSpy = new bootstrap.ScrollSpy(document.body, {
                target: '#navbar-top'
            });
        </script>
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-4LXC9BQBBX"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
        
          gtag('config', 'G-4LXC9BQBBX');
        </script>

        <!-- Javascript for showing and hiding the abstract and bibtex -->
        <script type="text/javascript">
            document.querySelectorAll(".links").forEach(function (p) {
                p.addEventListener("click", function (ev) {
                    // Make sure that the click is coming from a link
                    if (ev.target.nodeName != "A") {
                        return;
                    }
                    console.log(ev);

                    // Find the index of the div to toggle or return
                    var i = ev.target.dataset["index"];
                    if (i == undefined) {
                        return;
                    }

                    // Make sure to remove something else that was displayed
                    // and toggle the current one
                    Array.prototype.forEach.call(
                        ev.target.parentNode.children,
                        function (sibling) {
                            // We don't care about links etc
                            if (sibling.nodeName != "DIV") {
                                return;
                            }

                            // Hide others
                            if (sibling.dataset["index"] != i) {
                                sibling.style.display = "none";
                            }

                            // toggle the correct one
                            else {
                                if (sibling.style.display != "block") {
                                    sibling.style.display = "block";
                                } else {
                                    sibling.style.display = "none";
                                }
                            }
                        }
                    );
                    ev.preventDefault();
                });
            });
        </script>
    </body>
</html>
